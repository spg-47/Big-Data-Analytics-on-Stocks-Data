{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91d86e32",
   "metadata": {},
   "source": [
    "# Py-Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "528479b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "168dbf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f9b86ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark1 = SparkSession.builder.appName('Walmart_project').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "150e5d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark1.read.csv(\"walmart_stock.csv\",inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20989963",
   "metadata": {},
   "source": [
    "scenario 1:  print out first 5 columns ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d99c52e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|               Date|              Open|     High|      Low|             Close|  Volume|         Adj Close|\n",
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|2012-01-03 00:00:00|         59.970001|61.060001|59.869999|         60.330002|12668800|52.619234999999996|\n",
      "|2012-01-04 00:00:00|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|\n",
      "|2012-01-05 00:00:00|         59.349998|59.619999|58.369999|         59.419998|12768200|         51.825539|\n",
      "|2012-01-06 00:00:00|         59.419998|59.450001|58.869999|              59.0| 8069400|          51.45922|\n",
      "|2012-01-09 00:00:00|         59.029999|59.549999|58.919998|             59.18| 6679300|51.616215000000004|\n",
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada95732",
   "metadata": {},
   "source": [
    "scenario2: There are too many decimal places for mean and stddev in the describe() dataframe. Format the numbers to just show up to two decimal places.Pay careful attention to the datatypes that .describe() returns, we didn't cover how to do this exact formatting,but we covered something very similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ece9ca32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Date', 'timestamp'),\n",
       " ('Open', 'double'),\n",
       " ('High', 'double'),\n",
       " ('Low', 'double'),\n",
       " ('Close', 'double'),\n",
       " ('Volume', 'int'),\n",
       " ('Adj Close', 'double')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f808eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c5dc2597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|summary|              Open|             High|              Low|            Close|           Volume|        Adj Close|\n",
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|  count|              1258|             1258|             1258|             1258|             1258|             1258|\n",
      "|   mean| 72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|\n",
      "| stddev|  6.76809024470826|6.768186808159218|6.744075756255496|6.756859163732991|  4519780.8431556|6.722609449996857|\n",
      "|    min|56.389998999999996|        57.060001|        56.299999|        56.419998|          2094900|        50.363689|\n",
      "|    max|         90.800003|        90.970001|            89.25|        90.470001|         80898100|84.91421600000001|\n",
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "\n",
      "+-------+------+------+------+------+----------+---------+\n",
      "|summary|Open  |High  |Low   |Close |Volume    |Adj Close|\n",
      "+-------+------+------+------+------+----------+---------+\n",
      "|count  |1258.0|1258.0|1258.0|1258.0|1258.0    |1258.0   |\n",
      "|mean   |72.36 |72.84 |71.92 |72.39 |8222093.48|67.24    |\n",
      "|stddev |6.77  |6.77  |6.74  |6.76  |4519780.84|6.72     |\n",
      "|min    |56.39 |57.06 |56.3  |56.42 |2094900.0 |50.36    |\n",
      "|max    |90.8  |90.97 |89.25 |90.47 |8.08981E7 |84.91    |\n",
      "+-------+------+------+------+------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=df.describe()\n",
    "df1.show()\n",
    "for i in df1.columns[1:]:\n",
    "    df1=df1.withColumn(i,round(col(i),2).alias(i))\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e117e40a",
   "metadata": {},
   "source": [
    "scenario3: Create a new dataframe with a column called HV Ratio that is the ratio of the High Price versus volume of stock traded for a day.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "728ac6f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+-----+-----+--------+---------+---------+\n",
      "|Date               |Open |High |Low  |Close|Volume  |Adj Close|HV_Ratio |\n",
      "+-------------------+-----+-----+-----+-----+--------+---------+---------+\n",
      "|2012-01-03 00:00:00|59.97|61.06|59.87|60.33|12668800|52.62    |207481.16|\n",
      "|2012-01-04 00:00:00|60.21|60.35|59.47|59.71|9593300 |52.08    |158961.07|\n",
      "|2012-01-05 00:00:00|59.35|59.62|58.37|59.42|12768200|51.83    |214159.68|\n",
      "|2012-01-06 00:00:00|59.42|59.45|58.87|59.0 |8069400 |51.46    |135734.23|\n",
      "|2012-01-09 00:00:00|59.03|59.55|58.92|59.18|6679300 |51.62    |112162.89|\n",
      "|2012-01-10 00:00:00|59.43|59.71|58.98|59.04|6907300 |51.49    |115680.79|\n",
      "|2012-01-11 00:00:00|59.06|59.53|59.04|59.4 |6365600 |51.81    |106930.96|\n",
      "|2012-01-12 00:00:00|59.79|60.0 |59.4 |59.5 |7236400 |51.9     |120606.67|\n",
      "|2012-01-13 00:00:00|59.18|59.61|59.01|59.54|7729300 |51.93    |129664.48|\n",
      "|2012-01-17 00:00:00|59.87|60.11|59.52|59.85|8500000 |52.2     |141407.42|\n",
      "|2012-01-18 00:00:00|59.79|60.03|59.65|60.01|5911400 |52.34    |98474.1  |\n",
      "|2012-01-19 00:00:00|59.93|60.73|59.75|60.61|9234600 |52.86    |152059.94|\n",
      "|2012-01-20 00:00:00|60.75|61.25|60.67|61.01|10378800|53.21    |169449.8 |\n",
      "|2012-01-23 00:00:00|60.81|60.98|60.51|60.91|7134100 |53.13    |116990.82|\n",
      "|2012-01-24 00:00:00|60.75|62.0 |60.75|61.39|7362800 |53.54    |118754.84|\n",
      "+-------------------+-----+-----+-----+-----+--------+---------+---------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=df.withColumn('HV_Ratio',round(col('Volume')/col('High'),2))\n",
    "for i in df2.columns[1:]:\n",
    "    df2=df2.withColumn(i,round(col(i),2).alias(i))\n",
    "df2.show(15,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09769159",
   "metadata": {},
   "source": [
    "scenario4: What day had the Peak High in Price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "73b53c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-13 00:00:00\n"
     ]
    }
   ],
   "source": [
    "peak_high_day = df.select(\"Date\").orderBy(col(\"High\").desc()).first()[0]\n",
    "print(peak_high_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b302aef7",
   "metadata": {},
   "source": [
    "# Spark -SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1e62fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"walmart\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdf7d63",
   "metadata": {},
   "source": [
    "scenario5: What is the mean of the Close column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "573ddafc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       Close_Mean|\n",
      "+-----------------+\n",
      "|72.38844998012726|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark1.sql(\"Select mean(Close) as Close_Mean from walmart\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fffb58",
   "metadata": {},
   "source": [
    "scenario6:\n",
    "\n",
    "What is the max and min of the Volume column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cdce7138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|max_volume|min_volume|\n",
      "+----------+----------+\n",
      "|  80898100|   2094900|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark1.sql(\"Select max(volume) as max_volume,min(volume) as min_volume from walmart\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d134ea",
   "metadata": {},
   "source": [
    "scenario7:\n",
    "\n",
    "How many days was the Close lower than 60 dollars?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "010b4908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|total_days|\n",
      "+----------+\n",
      "|        81|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark1.sql(\"select count(date) as total_days from walmart where close<60\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c3ca66",
   "metadata": {},
   "source": [
    "Scenario8:\n",
    "\n",
    "What percentage of the time was the High greater than 80 dollars ?\n",
    "\n",
    "In other words, \n",
    "(Number of Days High>80)/(Total Days in the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f1f6027e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|percentage|\n",
      "+----------+\n",
      "|      9.14|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark1.sql(\"select round(sum(x)/count(x)*100,2) as percentage \\\n",
    "           from (select case when High>80 then 1 else 0 end as x from walmart) as s\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8880c2",
   "metadata": {},
   "source": [
    "Scenario9:\n",
    "\n",
    "What is the max High per year?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "87080716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|year| max_high|\n",
      "+----+---------+\n",
      "|2012|77.599998|\n",
      "|2013|81.370003|\n",
      "|2014|88.089996|\n",
      "|2015|90.970001|\n",
      "|2016|75.190002|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark1.sql(\"select year(date) as year , max(high) as max_high from walmart group by year(date) order by 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5eb361",
   "metadata": {},
   "source": [
    "# Visualization -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "fc89acd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+------------------+------------------+--------+------------------+\n",
      "|               Date|              Open|              High|               Low|             Close|  Volume|         Adj Close|\n",
      "+-------------------+------------------+------------------+------------------+------------------+--------+------------------+\n",
      "|2012-01-03 00:00:00|         59.970001|         61.060001|         59.869999|         60.330002|12668800|52.619234999999996|\n",
      "|2012-01-04 00:00:00|60.209998999999996|         60.349998|         59.470001|59.709998999999996| 9593300|         52.078475|\n",
      "|2012-01-05 00:00:00|         59.349998|         59.619999|         58.369999|         59.419998|12768200|         51.825539|\n",
      "|2012-01-06 00:00:00|         59.419998|         59.450001|         58.869999|              59.0| 8069400|          51.45922|\n",
      "|2012-01-09 00:00:00|         59.029999|         59.549999|         58.919998|             59.18| 6679300|51.616215000000004|\n",
      "|2012-01-10 00:00:00|             59.43|59.709998999999996|             58.98|59.040001000000004| 6907300|         51.494109|\n",
      "|2012-01-11 00:00:00|         59.060001|         59.529999|59.040001000000004|         59.400002| 6365600|         51.808098|\n",
      "|2012-01-12 00:00:00|59.790001000000004|              60.0|         59.400002|              59.5| 7236400|51.895315999999994|\n",
      "|2012-01-13 00:00:00|             59.18|59.610001000000004|59.009997999999996|59.540001000000004| 7729300|51.930203999999996|\n",
      "|2012-01-17 00:00:00|         59.869999|60.110001000000004|             59.52|         59.849998| 8500000|         52.200581|\n",
      "|2012-01-18 00:00:00|59.790001000000004|         60.029999|         59.650002|60.009997999999996| 5911400|         52.340131|\n",
      "|2012-01-19 00:00:00|             59.93|             60.73|             59.75|60.610001000000004| 9234600|         52.863447|\n",
      "|2012-01-20 00:00:00|             60.75|             61.25|         60.669998|61.009997999999996|10378800|53.212320999999996|\n",
      "|2012-01-23 00:00:00|         60.810001|             60.98|60.509997999999996|             60.91| 7134100|         53.125104|\n",
      "|2012-01-24 00:00:00|             60.75|              62.0|             60.75|61.389998999999996| 7362800| 53.54375400000001|\n",
      "|2012-01-25 00:00:00|             61.18|61.610001000000004|61.040001000000004|         61.470001| 5915800| 53.61353100000001|\n",
      "|2012-01-26 00:00:00|         61.799999|             61.84|             60.77|         60.970001| 7436200|         53.177436|\n",
      "|2012-01-27 00:00:00|60.860001000000004|         61.119999|60.540001000000004|60.709998999999996| 6287300|         52.950665|\n",
      "|2012-01-30 00:00:00|         60.470001|             61.32|         60.349998|         61.299999| 7636900|53.465256999999994|\n",
      "|2012-01-31 00:00:00|         61.529999|             61.57|         60.580002|61.360001000000004| 9761500|53.517590000000006|\n",
      "+-------------------+------------------+------------------+------------------+------------------+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145e617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to get no of rows in each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631f86b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import spark_partition_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "0fe25e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "27e3078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= df.repartition(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "dc7cd218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|partitionId|count|\n",
      "+-----------+-----+\n",
      "|          0|  420|\n",
      "|          1|  419|\n",
      "|          2|  419|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn(\"partitionId\",spark_partition_id()).groupBy(\"partitionId\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc032e99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142fe785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f03ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "\n",
    "# Connect to the Cloudera database\n",
    "conn = pyodbc.connect(\n",
    "    \"DRIVER={Cloudera ODBC Driver};\"\n",
    "    \"HOST=<hostname or IP address>;\"\n",
    "    \"PORT=<port number>;\"\n",
    "    \"DATABASE=<database name>;\"\n",
    "    \"UID=<username>;\"\n",
    "    \"PWD=<password>\"\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Execute SQL queries\n",
    "cursor.execute(\"SELECT * FROM your_table\")\n",
    "\n",
    "# Fetch and print the results\n",
    "rows = cursor.fetchall()\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8ff65f93",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o773.load.\n: java.sql.SQLException: No suitable driver\r\n\tat java.sql/java.sql.DriverManager.getDriver(DriverManager.java:300)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:107)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:107)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:171)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[139], line 21\u001b[0m\n\u001b[0;32m     13\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mque1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Load data from the Cloudera local database table\u001b[39;00m\n\u001b[0;32m     16\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpassword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdbtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m---> 21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Perform operations on the DataFrame\u001b[39;00m\n\u001b[0;32m     24\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mC:\\sparksetup\\spark-3.3.2-bin-hadoop3\\spark-3.3.2-bin-hadoop3\\python\\pyspark\\sql\\readwriter.py:184\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mC:\\sparksetup\\spark-3.3.2-bin-hadoop3\\spark-3.3.2-bin-hadoop3\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\sparksetup\\spark-3.3.2-bin-hadoop3\\spark-3.3.2-bin-hadoop3\\python\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\sparksetup\\spark-3.3.2-bin-hadoop3\\spark-3.3.2-bin-hadoop3\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o773.load.\n: java.sql.SQLException: No suitable driver\r\n\tat java.sql/java.sql.DriverManager.getDriver(DriverManager.java:300)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:107)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:107)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:39)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:34)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:171)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Cloudera Local Database Connection\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/path/to/cloudera/jdbc/driver.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the connection properties\n",
    "url = \"jdbc:cloudera://localhost:3306/project\"\n",
    "user = \"root\"\n",
    "password = \"cloudera\"\n",
    "table = \"que1\"\n",
    "\n",
    "# Load data from the Cloudera local database table\n",
    "df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"dbtable\", table) \\\n",
    "    .load()\n",
    "\n",
    "# Perform operations on the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Close the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7429239f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymysql\n",
      "  Downloading PyMySQL-1.1.0-py3-none-any.whl (44 kB)\n",
      "     -------------------------------------- 44.8/44.8 kB 734.6 kB/s eta 0:00:00\n",
      "Installing collected packages: pymysql\n",
      "Successfully installed pymysql-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pymysql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "86daa537",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "(2003, \"Can't connect to MySQL server on 'localhost' ([WinError 10061] No connection could be made because the target machine actively refused it)\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pymysql\\connections.py:644\u001b[0m, in \u001b[0;36mConnection.connect\u001b[1;34m(self, sock)\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 644\u001b[0m     sock \u001b[38;5;241m=\u001b[39m socket\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[0;32m    645\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect_timeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    646\u001b[0m     )\n\u001b[0;32m    647\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\socket.py:845\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 845\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    847\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\socket.py:833\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address)\u001b[0m\n\u001b[0;32m    832\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m--> 833\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[147], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m database \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproject\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Establish a connection\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[43mpymysql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Create a cursor object\u001b[39;00m\n\u001b[0;32m     14\u001b[0m cursor \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pymysql\\connections.py:358\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[1;34m(self, user, password, host, database, unix_socket, port, charset, collation, sql_mode, read_default_file, conv, use_unicode, client_flag, cursorclass, init_command, connect_timeout, read_default_group, autocommit, local_infile, max_allowed_packet, defer_connect, auth_plugin_map, read_timeout, write_timeout, bind_address, binary_prefix, program_name, server_public_key, ssl, ssl_ca, ssl_cert, ssl_disabled, ssl_key, ssl_verify_cert, ssl_verify_identity, compress, named_pipe, passwd, db)\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 358\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pymysql\\connections.py:711\u001b[0m, in \u001b[0;36mConnection.connect\u001b[1;34m(self, sock)\u001b[0m\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m DEBUG:\n\u001b[0;32m    710\u001b[0m         \u001b[38;5;28mprint\u001b[39m(exc\u001b[38;5;241m.\u001b[39mtraceback)\n\u001b[1;32m--> 711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# If e is neither DatabaseError or IOError, It's a bug.\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;66;03m# But raising AssertionError hides original error.\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;66;03m# So just reraise it.\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mOperationalError\u001b[0m: (2003, \"Can't connect to MySQL server on 'localhost' ([WinError 10061] No connection could be made because the target machine actively refused it)\")"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "\n",
    "# Define the connection details\n",
    "host = 'localhost'\n",
    "port = 3306\n",
    "user = 'root'\n",
    "password = 'cloudera'\n",
    "database = 'project'\n",
    "\n",
    "# Establish a connection\n",
    "conn = pymysql.connect(host=host, port=port, user=user, password=password, database=database)\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40634939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def func(n):\n",
    "    if n==1:\n",
    "        return 1\n",
    "    else:\n",
    "        return n*func(n-1)\n",
    "func(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
